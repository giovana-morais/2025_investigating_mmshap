Detected gpus, using --nv:
Quadro RTX 8000


hello :) your python: /ext3/miniconda3/envs/py310/bin/python
Python 3.10.18
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'QWenTokenizer'. 
The class this function is called from is 'CustomQwenTokenizer'.
The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
processing 1 questions from 12:13
audio_start_id: 155163, audio_end_id: 155164, audio_pad_id: 151851.
audio_start_id: 155163, audio_end_id: 155164, audio_pad_id: 151851.
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:03<00:27,  3.50s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:06<00:23,  3.42s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:10<00:20,  3.41s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:13<00:17,  3.43s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:17<00:13,  3.40s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:20<00:10,  3.37s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:23<00:06,  3.34s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:27<00:03,  3.35s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:29<00:00,  2.98s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:29<00:00,  3.24s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
